{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO:\\n\\n1. Bidirectional SE and TE\\n2. Give diffrent dags for the inverse RNN\\n3. multiple layers Encoder\\n4. masking for parallize training\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "\n",
    "from copy import deepcopy\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "from torchvision import datasets\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.bernoulli import Bernoulli \n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "\n",
    "1. Bidirectional SE and TE\n",
    "2. Give diffrent dags for the inverse RNN\n",
    "3. multiple layers Encoder\n",
    "4. masking for parallize training\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, i, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.i = i\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        attn = torch.bmm(q, k.transpose(1, 2)) # B, q_l, k_l\n",
    "    \n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf) # and after the softmax layer, the attn will be 0\n",
    "\n",
    "        if args.attn_softmax[self.i]:\n",
    "            attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn ,v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_q, d_k, i, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        n_head: repeat multiple times\n",
    "        '''\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_q = d_q\n",
    "        self.d_k = d_k\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_q)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k) # dk = dv != dq\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_k)\n",
    "        \n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_q)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5), i = i)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_k, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        d_q, d_k, n_head = self.d_q, self.d_k, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size() # sz_b = batch_size, and the _ = d_model?\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, self.d_q)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, self.d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, self.d_k)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, self.d_q) # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, self.d_k) # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, self.d_k) # (n*b) x lv x dv\n",
    "\n",
    "        # lk should be the same as lv\n",
    "        # Question: lq = lk = lv ? \n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1) # (n*b) x .. x .. , what if the mask = None\n",
    "            \n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, self.d_k) # the output dim should be the same as v\n",
    "        \n",
    "        # d_k = d_v = d_model/ n_head = 64 \n",
    "        # so n*d_v = d_model? trnasfer the dim back to d_model?\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1) # b x lq x (n*dk)\n",
    "        \n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual) \n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class GatedLinearUnit2D(nn.Module):\n",
    "    def __init__(self, i):\n",
    "        '''\n",
    "        for 2D\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.st_conv = nn.Conv1d(args.model_dim[i], args.gated_dim[i],1,1,0)\n",
    "        \n",
    "        self.end_conv = nn.Conv1d(args.gated_dim[i] // 2, args.model_dim[i],1,1,0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.st_conv(x)\n",
    "\n",
    "        a, b = x.split(x.size(1)//2, dim=1)\n",
    "        x = torch.tanh(a) * torch.sigmoid(b)\n",
    "        x = self.end_conv(x)\n",
    "        return x\n",
    "    \n",
    "class GatedLinearUnit1D(nn.Module):\n",
    "    def __init__(self, i):\n",
    "        '''\n",
    "        for 1D\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.st_Linear = nn.Linear(args.model_dim[i], args.gated_dim[i])\n",
    "        \n",
    "        self.end_Linear = nn.Linear(args.gated_dim[i] // 2, args.model_dim[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.st_Linear(x)\n",
    "\n",
    "        a, b = x.split(x.size(2)//2, dim=2)\n",
    "        x = torch.tanh(a) * torch.sigmoid(b)\n",
    "        x = self.end_Linear(x)\n",
    "        return x\n",
    "    \n",
    "class DilationConv1d(nn.Conv1d):\n",
    "    def __init__(self, kernel_size = 3, dilation = 1 , i = None):\n",
    "        padding = (kernel_size - 1) // 2 * dilation\n",
    "        super().__init__(args.model_dim[i], args.model_dim[i], kernel_size,\n",
    "                         padding = padding, dilation = dilation)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return super().forward(x)\n",
    "    \n",
    "    \n",
    "def cal_loss(pred, gold, smoothing):\n",
    "    \n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(Constants.PAD)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=Constants.PAD, reduction='sum')\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_gae(rewards, values, gamma=0.99, tau=0.95):\n",
    "    '''\n",
    "    Also use GAE in PPO.\n",
    "    '''\n",
    "    rewards = [0] * (values.size(0)-1) + [rewards]\n",
    "    \n",
    "    values = torch.cat([values,torch.tensor([[0.]]).to(device)],0)\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1]  - values[step] # Remove the gamma in here?\n",
    "        gae = delta + gamma * tau * gae # -value[step] will be used in the GAE step\n",
    "        returns.insert(0, gae + values[step]) # put the new one at the first, cuz we are calculating the A_GAE inversely.\n",
    "    return torch.stack(returns,0) # cuz we need the gradient of value, so we have to + value[step]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dags_translate(dags_four):\n",
    "\n",
    "    dags_ = deepcopy(dags_four)\n",
    "    \n",
    "    Act_fn_dict = {\n",
    "        \n",
    "        0: 'Sigmoid',\n",
    "        1: 'Tanh',\n",
    "        2: 'Softmax',\n",
    "        3: 'ReLU',\n",
    "        4: 'LeakyReLU',\n",
    "        5: 'Softplus'    \n",
    "    }\n",
    "\n",
    "    for dags in dags_:\n",
    "\n",
    "        for dag in dags:\n",
    "\n",
    "            if dag['Act_fn'] is not None:\n",
    "                \n",
    "                dag['Act_fn'] = Act_fn_dict[dag['Act_fn']] \n",
    "            \n",
    "            if dag['Put'] == 0:\n",
    "                dag['Put'] = 'Plus'\n",
    "            elif dag['Put'] == 1:\n",
    "                dag['Put'] = 'Emlement_Wise*'\n",
    "            elif dag['Put'] == 2:\n",
    "                dag['Put'] = 'Inplace'\n",
    "\n",
    "            dag['Output'] = bool(dag['Output'])\n",
    "            \n",
    "    return dags_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/.virtualenvs/python3.6/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n"
     ]
    }
   ],
   "source": [
    "class args(object):\n",
    "    \n",
    "    \n",
    "    # Order SourceEncoder, TargetEncoder, \n",
    "    \n",
    "    # SourceEncoder\n",
    "    \n",
    "    model_dim = [512] * 4\n",
    "    attn_softmax = [True] * 4\n",
    "    head = [5] * 4 \n",
    "    \n",
    "    attn = [5] * 4\n",
    "    attn_dim = [64] * 4\n",
    "    drop_r = [0.1] * 4\n",
    "    bns = [5] * 4\n",
    "    opts = [5] * 4\n",
    "    opts_types = [2,2,4,4]\n",
    "    early_output = [True] * 4\n",
    "    act_fns = [6] * 4\n",
    "    put = [3] * 4\n",
    "    \n",
    "    \n",
    "    \n",
    "    num_hidden_cells = [5, 5] # use decoder to output the softmax for generating the next result\n",
    "    num_input_cells = [5, 5] # if the input is just a single tensor then repeat, if the input is a list then use the list\n",
    "    # send hidden cells to the next, the input cells will be leaf at that time step\n",
    "    \n",
    "    gated_dim = model_dim\n",
    "    \n",
    "    C_opts = [ o*t for o, t in zip(opts, opts_types)]\n",
    "    \n",
    "    RNN_emb_dim = [256] * 4 \n",
    "    \n",
    "    # None has to be added to the list\n",
    "    \n",
    "\n",
    "    \n",
    "    num_outputs = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        num_outputs.append([num_hidden_cells[i] + num_input_cells[i]]*4 + [C_opts[i] + 1] *4 + \\\n",
    "                           [attn[i]+1] + [(bns[i]*2)+1] + [act_fns[i] + 1] + [num_hidden_cells[i]+num_input_cells[i]] + \\\n",
    "                           [put[i]] + [2]\n",
    "                          )\n",
    "\n",
    "    num_outputs.append([num_hidden_cells[1]] + [num_hidden_cells[0]]*2 + [C_opts[2] + 1] * 4 + \\\n",
    "                       [attn[2]] + [(bns[i]*3)+1] + [act_fns[2]+1] + [num_hidden_cells[1]] + \\\n",
    "                       [put[2]] + [2]\n",
    "                       ) \n",
    "    \n",
    "    num_outputs.append([num_hidden_cells[1]]*4 + [C_opts[3] + 1] *4 + \\\n",
    "                       [attn[3]+1] + [(bns[i]*3)+1] + [act_fns[3]+1] + [num_hidden_cells[1]] + \\\n",
    "                       [put[2]] + [2] + [opts[3]]\n",
    "                       )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     step_names = ['Retrieve_cell','Retrieve_cell_1','Retrieve_cell_2','Retrieve_cell_3',\n",
    "#              'Operation', 'Operation_1', 'Operation_2', 'Operation_3',\n",
    "#              'Attn', 'Norm', 'Act_fn', 'Return_cell', 'Put', 'Output', 'Out_opt']\n",
    "    \n",
    "    step_names = [['Retrieve_cell','Retrieve_cell_1','Retrieve_cell_2','Retrieve_cell_3',\n",
    "                 'Operation', 'Operation_1', 'Operation_2', 'Operation_3',\n",
    "                 'Attn', 'Norm', 'Act_fn', 'Return_cell', 'Put', 'Output'],\n",
    "                  ['Retrieve_cell','Retrieve_cell_1','Retrieve_cell_2','Retrieve_cell_3',\n",
    "                 'Operation', 'Operation_1', 'Operation_2', 'Operation_3',\n",
    "                 'Attn', 'Norm', 'Act_fn', 'Return_cell', 'Put', 'Output'],\n",
    "                  ['Retrieve_cell_1','Retrieve_cell_2','Retrieve_cell_3',\n",
    "                 'Operation', 'Operation_1', 'Operation_2', 'Operation_3',\n",
    "                 'Attn', 'Norm', 'Act_fn', 'Return_cell', 'Put', 'Output'],\n",
    "                 ['Retrieve_cell','Retrieve_cell_1','Retrieve_cell_2','Retrieve_cell_3',\n",
    "                 'Operation', 'Operation_1', 'Operation_2', 'Operation_3',\n",
    "                 'Attn', 'Norm', 'Act_fn', 'Return_cell', 'Put', 'Output', 'Out_opt']\n",
    "                 ]\n",
    "    \n",
    "    none_opt_names = ['Operation','Attn','Norm','Act_fn'] # we must have out_opt to project the tensor to T_vocab_size\n",
    "    \n",
    "    RNN = ['LSTM']*4\n",
    "    \n",
    "    assert RNN[0] == RNN[1], 'SE RNN has to be the same as TE RNN'\n",
    "    \n",
    "    \n",
    "    \n",
    "    num_steps = [ len(names) for names in  step_names]\n",
    "    \n",
    "    names = ['SourceEncoder','TargetEncoder','ThinkingCell','Decoder']\n",
    "    \n",
    "    \n",
    "    ### Data \n",
    "    \n",
    "    batch_size = 5\n",
    "    \n",
    "    \n",
    "    root = '../Data/multi30k/data/'\n",
    "\n",
    "    # torchtext.datasets.Multi30k.download(root)\n",
    "\n",
    "    DE = torchtext.data.Field(include_lengths=True,\n",
    "               init_token='<sos>', eos_token='<eos>')\n",
    "    EN = torchtext.data.Field(include_lengths=True,\n",
    "               init_token='<sos>', eos_token='<eos>')\n",
    "    \n",
    "    # train, val, test = torchtext.datasets.Multi30k.splits(exts=('.de', '.en'), fields=(DE, EN), root = '../Data/multi30k/data')\n",
    "\n",
    "    train, val, test = torchtext.datasets.TranslationDataset.splits(      \n",
    "          path = root,  \n",
    "          exts = ['.de', '.en'],   \n",
    "          fields = [('src', DE), ('trg', EN)],\n",
    "          train = 'train', \n",
    "          validation = 'val', \n",
    "          test = 'test2016')\n",
    "    DE.build_vocab(train.src, min_freq=2)\n",
    "    EN.build_vocab(train.trg, max_size=10000)\n",
    "    train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "            (train, val, test), batch_size=batch_size, repeat=False)\n",
    "\n",
    "    max_shape1 = 0 \n",
    "    for iterator in [train_iter, val_iter]:\n",
    "        for i in iterator:\n",
    "            for inst in [i.trg, i.src]:\n",
    "                if inst[0].shape[0] > max_shape1:   \n",
    "                    max_shape1 = inst[0].shape[0]\n",
    "\n",
    "    \n",
    "    S_vocab_size = len(DE.vocab)\n",
    "    T_vocab_size = len(EN.vocab)\n",
    "    \n",
    "#     # Controller\n",
    "    \n",
    "    \n",
    "    num_layers = [5,5]\n",
    "    \n",
    "    label_smooth = True\n",
    "    \n",
    "    emb_dim = model_dim[0]\n",
    "    RNN_dim = [256] * 4\n",
    "    RNN_dropout = [0.1] * 4\n",
    "    \n",
    "    log_softmax = [False] * 4 \n",
    "    \n",
    "    \n",
    "    shared_lr = 0.0002\n",
    "    controller_lr = 0.0002\n",
    "    \n",
    "    eps = 1e-8\n",
    "    discount = 0.9\n",
    "    decay = 0.95\n",
    "    epoch = 150\n",
    "\n",
    "    shared_maxsteps = 5 # 400\n",
    "    controller_maxsteps = 5 # 2000\n",
    "    \n",
    "    controller_sch = False\n",
    "    controller_sch_steps = 2000\n",
    "    controller_sch_gamma = 0.75\n",
    "\n",
    "    shared_sch = False\n",
    "    shared_sch_steps = 2000\n",
    "    shared_sch_gamma = 0.75\n",
    "    \n",
    "    shared_grad_norm = 0.25\n",
    "    controller_grad_norm = 0.25\n",
    "    \n",
    "    controller_L2 = 0\n",
    "    shared_L2 = 1e-7\n",
    "\n",
    "    reward_c = 80\n",
    "    \n",
    "    agent = 'ActorCritic' # [ActorCritic, PolicyGradient]\n",
    "    \n",
    "    baseline_decay = 0.95\n",
    "    val_test_steps = 5\n",
    "    \n",
    "    critic_coef = 0.5\n",
    "    entropy_coef = 0\n",
    "    \n",
    "    model_name = 'FetchingNAS' \n",
    "    model_path ='./'+ model_name +'/Model/'\n",
    "\n",
    "\n",
    "\n",
    "class Constants():\n",
    "    \n",
    "    PAD = 1\n",
    "    UNK = 0\n",
    "    SOS = 2\n",
    "    EOS = 3\n",
    "\n",
    "    PAD_WORD = '<blank>'\n",
    "    UNK_WORD = '<UNK>'\n",
    "    SOS_WORD = '<SOS>'\n",
    "    EOS_WORD = '<EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, i):\n",
    "        \n",
    "        super().__init__()\n",
    "        '''\n",
    "        This should be a RNN with Customize RNN Cells\n",
    "        \n",
    "        So In Every Step, the cell will recerive a hidden state and an input.\n",
    "        \n",
    "        it's a 2D input, so the Self-Attn can't be used.\n",
    "        \n",
    "        function can be used :\n",
    "        \n",
    "        The Bmm can't be used, but it need an activation followed by or before the bmm.\n",
    "        \n",
    "        no bmm since we only have two dims, but the elementwise should be allowed\n",
    "        \n",
    "        Linear\n",
    "        \n",
    "        element-wise, bmm will be included\n",
    "        \n",
    "        Activation Function:\n",
    "        \n",
    "        Softmax, ReLU, Tanh, Sigmoid, LeakyReLU, \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.i = i\n",
    "\n",
    "        self.attns = nn.ModuleList()\n",
    "        self.attns.extend([MultiHeadAttention(args.head[i], args.model_dim[i], args.attn_dim[i], args.attn_dim[i], i, args.drop_r[i]) for _ in range(args.attn[i])])\n",
    "        \n",
    "        if args.bns[i] > 0:\n",
    "            self.norms = nn.ModuleList()\n",
    "            self.norms.append(nn.BatchNorm1d(args.model_dim[i], affine = False))\n",
    "            self.norms.append(nn.LayerNorm((args.model_dim[i]), elementwise_affine= False))\n",
    "    \n",
    "            self.norms.extend([nn.BatchNorm1d(args.model_dim[i]) for _ in range(args.bns[i]-1)])\n",
    "            self.norms.extend([nn.LayerNorm((args.model_dim[i])) for _ in range(args.bns[i]-1)])\n",
    "        \n",
    "        self.opts = nn.ModuleList()\n",
    "        self.opts.extend([nn.Linear(args.model_dim[i], args.model_dim[i]) for _ in range(args.opts[i])])\n",
    "        self.opts.extend([GatedLinearUnit1D(i) for _ in range(args.opts[i])])\n",
    "        \n",
    "        self.act_fns = nn.ModuleList()\n",
    "        self.act_fns.extend([nn.Sigmoid(),\n",
    "                             nn.Tanh(),\n",
    "                             nn.Softmax(dim = -1),\n",
    "                             nn.ReLU(inplace = True),\n",
    "                             nn.LeakyReLU(0.2, inplace = True,),\n",
    "                             nn.Softplus()\n",
    "                            ])\n",
    "        \n",
    "    \n",
    "    def forward(self, x, hidden, dags):\n",
    "        \n",
    "        \n",
    "        # the difference of the simple and the blocked algorithm is the simple alg\n",
    "        \n",
    "        # will be more \n",
    "        \n",
    "        \n",
    "        if torch.is_tensor(x):\n",
    "            \n",
    "            if x.dim() == 2:\n",
    "                \n",
    "                x = x.unsqueeze(1)\n",
    "            \n",
    "            cells = [x] * (args.num_input_cells[self.i])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            cells = x  \n",
    "        \n",
    "        if hidden is None:\n",
    "            \n",
    "            assert torch.is_tensor(x)\n",
    "            \n",
    "            # does that mean the hidden state of the RNN has to the same as the embedding dim? \n",
    "            \n",
    "            hidden = torch.zeros_like(x)\n",
    "            \n",
    "            hidden = [hidden] * (args.num_hidden_cells[self.i])\n",
    "            \n",
    "        cells = hidden + cells\n",
    "        \n",
    "        args.c = cells\n",
    "\n",
    "#         B,D = cells[0].shape # it's a 2D tensor\n",
    "        \n",
    "        # return cells, can be send cells to the next? \n",
    "        \n",
    "        # must interact with the new character,\n",
    "        \n",
    "        # what if we send the hidden state to the next cell, \n",
    "        \n",
    "        # and the return cell can only be selected from certain cells ? \n",
    "        \n",
    "        # only send certain amount of cells to the next \n",
    "    \n",
    "        for i, dag in enumerate(dags):\n",
    "            \n",
    "            args.d = dag\n",
    "            \n",
    "            opting_cell = cells[dag['Retrieve_cell']].clone()\n",
    "            \n",
    "            if not dag['Attn'] is None:\n",
    "            \n",
    "                opting_cell_q = cells[dag['Retrieve_cell_1']].clone() # q is from Target\n",
    "                opting_cell_k = cells[dag['Retrieve_cell_2']].clone()\n",
    "                opting_cell_v = cells[dag['Retrieve_cell_3']].clone()\n",
    "            \n",
    "                if not dag['Operation_1'] is None:\n",
    "                    opting_cell_q = self.opts[dag['Operation_1']](opting_cell_q)\n",
    "                if not dag['Operation_2'] is None:\n",
    "                    opting_cell_k = self.opts[dag['Operation_2']](opting_cell_k)\n",
    "                if not dag['Operation_3'] is None:\n",
    "                    opting_cell_v = self.opts[dag['Operation_3']](opting_cell_v)\n",
    "                    \n",
    "                # Use Clone? \n",
    "                opting_cell = self.attns[dag['Attn']](opting_cell_q, opting_cell_k, opting_cell_v)[0]  # use clone?\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                opting_cell = cells[dag['Retrieve_cell']].clone()\n",
    "                \n",
    "            if not dag['Operation'] is None:\n",
    "                opting_cell = self.opts[dag['Operation']](opting_cell)\n",
    "            if not dag['Norm'] is None: \n",
    "                if type(self.norms[dag['Norm']]) in [nn.BatchNorm1d, nn.InstanceNorm1d]:\n",
    "                    opting_cell = self.norms[dag['Norm']](opting_cell.permute(0,2,1)).permute(0,2,1)\n",
    "                elif type(self.norms[dag['Norm']]) in [nn.LayerNorm]:\n",
    "                    opting_cell = self.norms[dag['Norm']](opting_cell)\n",
    "            if not dag['Act_fn'] is None:\n",
    "                opting_cell = self.act_fns[dag['Act_fn']](opting_cell)\n",
    "            if dag['Put'] == 0:\n",
    "                cells[dag['Return_cell']] = cells[dag['Return_cell']] + opting_cell\n",
    "            elif dag['Put'] == 1:\n",
    "                cells[dag['Return_cell']] = cells[dag['Return_cell']] * opting_cell\n",
    "            elif dag['Put'] == 2:\n",
    "                cells[dag['Return_cell']] = opting_cell\n",
    "            else: \n",
    "                raise NotImplementedError\n",
    "                \n",
    "            # controlling nan output\n",
    "            # checking the last operation\n",
    "            if torch.isnan(cells[dag['Return_cell']]).sum() > 0:\n",
    "                print('The NaN portion: ', (torch.isnan(cells[dag['Return_cell']]).sum()/cells[dag['Return_cell']].nelement()))\n",
    "                print('The Inf portion: ', (torch.isinf(cells[dag['Return_cell']]).sum()/cells[dag['Return_cell']].nelement()))\n",
    "            \n",
    "                # what if we don't use the mask rather than canceal the loss.backward()\n",
    "                cells[dag['Return_cell']][torch.isnan(cells[dag['Return_cell']])] = 0 # masking nan\n",
    "                cells[dag['Return_cell']][torch.isinf(cells[dag['Return_cell']])] = 0 # maksing inf\n",
    "                \n",
    "            # check inf as well, if the loss is inf, then set the reward to 0\n",
    "\n",
    "            \n",
    "            if args.early_output[self.i] and dag['Output']:\n",
    "                return cells[:args.num_hidden_cells[self.i]]\n",
    "            \n",
    "        return cells[:args.num_hidden_cells[self.i]] # send the hidden state to next\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, i, bidirectional = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.i = i\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.cell = EncodingCell(i)\n",
    "        \n",
    "    def forward(self, x, hidden = None, dags = None):\n",
    "        \n",
    "        B,S = x.shape[:-1]\n",
    "        \n",
    "        h_= []\n",
    "\n",
    "        for j in range(S):\n",
    "            hidden = self.cell(x[:,j,:], hidden, dags)  # the dim of length will be remove automatically\n",
    "            h_.append(hidden)\n",
    "            \n",
    "        h_ = [torch.stack(h,1).squeeze() for h in zip(*h_)]\n",
    "            \n",
    "        if self.bidirectional:\n",
    "            \n",
    "            h_inver = []\n",
    "            \n",
    "            hidden = None\n",
    "            \n",
    "            for j in reversed(range(S)):\n",
    "                \n",
    "                hidden = self.cell(x[:,j,:], hidden, dags)\n",
    "                \n",
    "                h_inver.append(hidden)\n",
    "           \n",
    "            h_ += [torch.stack(h,1).squeeze() for h in zip(*h_inver)]\n",
    "            \n",
    "        return h_\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only send the last hidden state to the transformer or I should record all the hidden states? \n",
    "\n",
    "# how can I record all, if I record it in the hidden state, then the input of thinkingCell will become\n",
    "\n",
    "# B,L,D, multiple cells can be considered a larger batch_size\n",
    "\n",
    "# get a list cells = [ ] (len = args.num_hidden_cells * L) \n",
    "\n",
    "# B* args.num_cells, L, D\n",
    "\n",
    "# use hidden state in the Decoder\n",
    "\n",
    "class ThinkingCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, i):\n",
    "        \n",
    "        super().__init__()\n",
    "        '''\n",
    "\n",
    "        input will be a 3D tensor(B* args.num_cells, L, D) 各個cell裡面的自己排列 則 [(B, L, D)] * args.num_cells\n",
    "        \n",
    "        so num_cells in the thinkingcell will be the same as encoding cells? \n",
    "        \n",
    "        combine to (B, TargetL, D)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.i = i\n",
    "        \n",
    "        self.attns = nn.ModuleList()\n",
    "        self.attns.extend([MultiHeadAttention(args.head[i], args.model_dim[i], args.attn_dim[i], args.attn_dim[i], i, args.drop_r[i]) for _ in range(args.attn[i])])\n",
    "        \n",
    "        if args.bns[i] > 0:\n",
    "            \n",
    "            self.norms = nn.ModuleList()\n",
    "            self.norms.append(nn.BatchNorm1d(args.model_dim[i], affine = False))\n",
    "            self.norms.append(nn.LayerNorm(args.model_dim[i], elementwise_affine= False))\n",
    "            self.norms.append(nn.InstanceNorm1d(args.model_dim[i], affine = False))\n",
    "    \n",
    "            self.norms.extend([nn.BatchNorm1d(args.model_dim[i]) for _ in range(args.bns[i]-1)])\n",
    "            self.norms.extend([nn.LayerNorm(args.model_dim[i]) for _ in range(args.bns[i]-1)])\n",
    "            self.norms.extend([nn.InstanceNorm1d(args.model_dim[i]) for _ in range(args.bns[i]-1)])\n",
    "        \n",
    "        self.opts = nn.ModuleList()\n",
    "        self.opts.extend([DilationConv1d(kernel_size = 1, dilation = 1, i = i) for _ in range(args.opts[i])]) # this one can be seen as Linear\n",
    "        self.opts.extend([DilationConv1d(kernel_size = 3, dilation = 1, i = i) for _ in range(args.opts[i])])\n",
    "        self.opts.extend([DilationConv1d(kernel_size = 5, dilation = 1, i = i) for _ in range(args.opts[i])])\n",
    "        self.opts.extend([GatedLinearUnit2D(i) for _ in range(args.opts[i])])\n",
    "        \n",
    "        # find one opt for query and another one for key and value. \n",
    "        \n",
    "        # only use attention in the thinking process? make the length to become as Target\n",
    "        \n",
    "        # Operation_1 and Opearation_2\n",
    "        \n",
    "        # use Operation 2 on the, set q, K, V as different input?, therefore, 3 different? \n",
    "        \n",
    "        # give this different option\n",
    "        \n",
    "        # it will become like a transformer\n",
    "        \n",
    "        self.act_fns = nn.ModuleList()\n",
    "        self.act_fns.extend([nn.Sigmoid(),\n",
    "                             nn.Tanh(),\n",
    "                             nn.Softmax(dim = -1),\n",
    "                             nn.ReLU(inplace = True),\n",
    "                             nn.LeakyReLU(0.2, inplace = True,),\n",
    "                             nn.Softplus()\n",
    "                            ])\n",
    "        \n",
    "    \n",
    "    def forward(self, SE_input, TE_input, dags):\n",
    "        \n",
    "        args.ccc = SE_input + TE_input\n",
    "        \n",
    "        \n",
    "        # the difference of the simple and the blocked algorithm is the simple alg\n",
    "        # will be more \n",
    "        \n",
    "#         B,D = cells[0].shape # it's a 2D tensor\n",
    "        \n",
    "        # return cells, can be send cells to the next? \n",
    "        \n",
    "        # must interact with the new character,\n",
    "        \n",
    "        # what if we send the hidden state to the next cell, \n",
    "        \n",
    "        # and the return cell can only be selected from certain cells ? \n",
    "        \n",
    "        # only send certain amount of cells to the next \n",
    "        \n",
    "#         cells = SE_input + TE_input\n",
    "    \n",
    "        for i, dag in enumerate(dags):\n",
    "            \n",
    "            # B,S,D\n",
    "            \n",
    "            #opt-> B.D.S\n",
    "            \n",
    "            args.td = dag\n",
    "            opting_cell_q = TE_input[dag['Retrieve_cell_1']].clone() # q is from Target\n",
    "            opting_cell_k = SE_input[dag['Retrieve_cell_2']].clone()\n",
    "            opting_cell_v = SE_input[dag['Retrieve_cell_3']].clone()\n",
    "\n",
    "            if not dag['Operation_1'] is None:\n",
    "                opting_cell_q = self.opts[dag['Operation_1']](opting_cell_q.permute(0,2,1)).permute(0,2,1)\n",
    "                \n",
    "            if not dag['Operation_2'] is None:\n",
    "                opting_cell_k = self.opts[dag['Operation_2']](opting_cell_k.permute(0,2,1)).permute(0,2,1)\n",
    "            if not dag['Operation_3'] is None:\n",
    "                opting_cell_v = self.opts[dag['Operation_3']](opting_cell_v.permute(0,2,1)).permute(0,2,1)\n",
    "\n",
    "            opting_cell = self.attns[dag['Attn']](opting_cell_q, opting_cell_k, opting_cell_v)[0] # use clone?\n",
    "            \n",
    "            if not dag['Operation'] is None:\n",
    "                opting_cell = self.opts[dag['Operation']](opting_cell.permute(0,2,1)).permute(0,2,1)\n",
    "            if not dag['Norm'] is None: \n",
    "                if type(self.norms[dag['Norm']]) in [nn.BatchNorm1d, nn.InstanceNorm1d]:\n",
    "                    opting_cell = self.norms[dag['Norm']](opting_cell.permute(0,2,1)).permute(0,2,1)\n",
    "                elif type(self.norms[dag['Norm']]) in [nn.LayerNorm]:\n",
    "                    opting_cell = self.norms[dag['Norm']](opting_cell)\n",
    "            if not dag['Act_fn'] is None:\n",
    "                opting_cell = self.act_fns[dag['Act_fn']](opting_cell)\n",
    "            if dag['Put'] == 0:\n",
    "                TE_input[dag['Return_cell']] = TE_input[dag['Return_cell']] + opting_cell\n",
    "            elif dag['Put'] == 1:\n",
    "                TE_input[dag['Return_cell']] = TE_input[dag['Return_cell']] * opting_cell\n",
    "            elif dag['Put'] == 2:\n",
    "                TE_input[dag['Return_cell']] = opting_cell\n",
    "            else: \n",
    "                raise NotImplementedError\n",
    "                \n",
    "            # controlling nan output\n",
    "            # checking the last operation\n",
    "            if torch.isnan(TE_input[dag['Return_cell']]).sum() > 0:\n",
    "                print('The NaN portion: ', (torch.isnan(TE_input[dag['Return_cell']]).sum()/TE_input[dag['Return_cell']].nelement()))\n",
    "                print('The Inf portion: ', (torch.isinf(TE_input[dag['Return_cell']]).sum()/TE_input[dag['Return_cell']].nelement()))\n",
    "            \n",
    "                # what if we don't use the mask rather than canceal the loss.backward()\n",
    "                TE_input[dag['Return_cell']][torch.isnan(TE_input[dag['Return_cell']])] = 0 # masking nan\n",
    "                TE_input[dag['Return_cell']][torch.isinf(TE_input[dag['Return_cell']])] = 0 # maksing inf\n",
    "                \n",
    "            # check inf as well, if the loss is inf, then set the reward to 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            # should i give a output selection? \n",
    "            \n",
    "            if args.early_output[self.i] and dag['Output']:\n",
    "                return TE_input[dag['Return_cell']]\n",
    "            \n",
    "        return TE_input# send the hidden state to next\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, i):\n",
    "        \n",
    "        super().__init__()\n",
    "        '''\n",
    "\n",
    "        input will be a 3D tensor(B* args.num_cells, L, D) 各個cell裡面的自己排列 則 [(B, L, D)] * args.num_cells\n",
    "        \n",
    "        so num_cells in the thinkingcell will be the same as encoding cells? \n",
    "        \n",
    "        combine to (B, TargetL, D)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.i = i\n",
    "        \n",
    "        self.attns = nn.ModuleList()\n",
    "        self.attns.extend([MultiHeadAttention(args.head[i], args.model_dim[i], args.attn_dim[i], args.attn_dim[i], i, args.drop_r[i]) for _ in range(args.attn[i]+1)])\n",
    "        \n",
    "        if args.bns[i] > 0:\n",
    "            \n",
    "            self.norms = nn.ModuleList()\n",
    "            self.norms.append(nn.BatchNorm1d(args.model_dim[i], affine = False))\n",
    "            self.norms.append(nn.LayerNorm(args.model_dim[i], elementwise_affine= False))\n",
    "            self.norms.append(nn.InstanceNorm1d(args.model_dim[i], affine = False))\n",
    "    \n",
    "            self.norms.extend([nn.BatchNorm1d(args.model_dim[i]) for _ in range(args.bns[i]-1)])\n",
    "            self.norms.extend([nn.LayerNorm(args.model_dim[i]) for _ in range(args.bns[i]-1)])\n",
    "            self.norms.extend([nn.InstanceNorm1d(args.model_dim[i]) for _ in range(args.bns[i]-1)])\n",
    "        \n",
    "        self.opts = nn.ModuleList()\n",
    "        self.opts.extend([DilationConv1d(kernel_size = 1, dilation = 1, i = i) for _ in range(args.opts[i])]) # this one can be seen as Linear\n",
    "        self.opts.extend([DilationConv1d(kernel_size = 3, dilation = 1, i = i) for _ in range(args.opts[i])])\n",
    "        self.opts.extend([DilationConv1d(kernel_size = 5, dilation = 1, i = i) for _ in range(args.opts[i])])\n",
    "        self.opts.extend([GatedLinearUnit2D(i) for _ in range(args.opts[i])])\n",
    "        \n",
    "        # only give linear for decoding? \n",
    "        self.out = nn.ModuleList()\n",
    "        self.out.extend([nn.Linear(args.model_dim[i], args.T_vocab_size) for _ in range(args.opts[i])])\n",
    "        self.sm = nn.Softmax(-1)\n",
    "        \n",
    "        # find one opt for query and another one for key and value. \n",
    "        \n",
    "        # only use attention in the thinking process? make the length to become as Target\n",
    "        \n",
    "        # Operation_1 and Opearation_2\n",
    "        \n",
    "        # use Operation 2 on the, set q, K, V as different input?, therefore, 3 different? \n",
    "        \n",
    "        # give this different option\n",
    "        \n",
    "        # it will become like a transformer\n",
    "        \n",
    "        self.act_fns = nn.ModuleList()\n",
    "        self.act_fns.extend([nn.Sigmoid(),\n",
    "                             nn.Tanh(),\n",
    "                             nn.Softmax(dim = -1),\n",
    "                             nn.ReLU(inplace = True),\n",
    "                             nn.LeakyReLU(0.2, inplace = True,),\n",
    "                             nn.Softplus()\n",
    "                            ])\n",
    "          \n",
    "        \n",
    "    def forward(self, TE_input, dags):\n",
    "        \n",
    "        \n",
    "        # the difference of the simple and the blocked algorithm is the simple alg\n",
    "        # will be more \n",
    "        \n",
    "#         B,D = cells[0].shape # it's a 2D tensor\n",
    "        \n",
    "        # return cells, can be send cells to the next? \n",
    "        \n",
    "        # must interact with the new character,\n",
    "        \n",
    "        # what if we send the hidden state to the next cell, \n",
    "        \n",
    "        # and the return cell can only be selected from certain cells ? \n",
    "        \n",
    "        # only send certain amount of cells to the next \n",
    "        \n",
    "#         cells = SE_input + TE_input\n",
    "    \n",
    "        for i, dag in enumerate(dags):\n",
    "            \n",
    "            args.de = dag\n",
    "            args.te = TE_input\n",
    "            \n",
    "            if not dag['Attn'] is None:\n",
    "            \n",
    "                opting_cell_q = TE_input[dag['Retrieve_cell_1']].clone() # q is from Target\n",
    "                opting_cell_k = TE_input[dag['Retrieve_cell_2']].clone()\n",
    "                opting_cell_v = TE_input[dag['Retrieve_cell_3']].clone()\n",
    "            \n",
    "                if not dag['Operation_1'] is None:\n",
    "                    opting_cell_q = self.opts[dag['Operation_1']](opting_cell_q.permute(0,2,1)).permute(0,2,1)\n",
    "                if not dag['Operation_2'] is None:\n",
    "                    opting_cell_k = self.opts[dag['Operation_2']](opting_cell_k.permute(0,2,1)).permute(0,2,1)\n",
    "                if not dag['Operation_3'] is None:\n",
    "                    opting_cell_v = self.opts[dag['Operation_3']](opting_cell_v.permute(0,2,1)).permute(0,2,1)\n",
    "                    \n",
    "                opting_cell = self.attns[dag['Attn']](opting_cell_q, opting_cell_k, opting_cell_v)[0] # use clone?\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                opting_cell = TE_input[dag['Retrieve_cell']].clone()\n",
    "                \n",
    "            if not dag['Operation'] is None:\n",
    "                    opting_cell = self.opts[dag['Operation']](opting_cell.permute(0,2,1)).permute(0,2,1)\n",
    "                    \n",
    "            if not dag['Norm'] is None: \n",
    "                if type(self.norms[dag['Norm']]) in [nn.BatchNorm1d, nn.InstanceNorm1d]:\n",
    "                    opting_cell = self.norms[dag['Norm']](opting_cell.permute(0,2,1)).permute(0,2,1)\n",
    "                elif type(self.norms[dag['Norm']]) in [nn.LayerNorm]:\n",
    "                    opting_cell = self.norms[dag['Norm']](opting_cell)\n",
    "            if not dag['Act_fn'] is None:\n",
    "                opting_cell = self.act_fns[dag['Act_fn']](opting_cell)\n",
    "            if dag['Put'] == 0:\n",
    "                TE_input[dag['Return_cell']] = TE_input[dag['Return_cell']] + opting_cell\n",
    "            elif dag['Put'] == 1:\n",
    "                TE_input[dag['Return_cell']] = TE_input[dag['Return_cell']] * opting_cell\n",
    "            elif dag['Put'] == 2:\n",
    "                TE_input[dag['Return_cell']] = opting_cell\n",
    "            else: \n",
    "                raise NotImplementedError\n",
    "                \n",
    "            # controlling nan output\n",
    "            # checking the last operation\n",
    "            if torch.isnan(TE_input[dag['Return_cell']]).sum() > 0:\n",
    "                \n",
    "                print('The NaN portion: ', (torch.isnan(TE_input[dag['Return_cell']]).sum()/TE_input[dag['Return_cell']].nelement()))\n",
    "                print('The Inf portion: ', (torch.isinf(TE_input[dag['Return_cell']]).sum()/TE_input[dag['Return_cell']].nelement()))\n",
    "            \n",
    "                # what if we don't use the mask rather than canceal the loss.backward()\n",
    "                TE_input[dag['Return_cell']][torch.isnan(TE_input[dag['Return_cell']])] = 0 # masking nan\n",
    "                TE_input[dag['Return_cell']][torch.isinf(TE_input[dag['Return_cell']])] = 0 # maksing inf\n",
    "                \n",
    "            # check inf as well, if the loss is inf, then set the reward to 0\n",
    "\n",
    "            if args.early_output[self.i] and dag['Output']: # Out_opt can't be None\n",
    "                return self.sm(self.out[dag['Out_opt']](TE_input[dag['Return_cell']]))\n",
    "            \n",
    "        return self.sm(self.out[dag['Out_opt']](TE_input[dag['Return_cell']])) # send the hidden state to next\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \n",
    "    def __init__(self, i):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.i = i\n",
    "            \n",
    "        self.embs = nn.ModuleList([nn.Embedding(n, args.RNN_emb_dim[i]) for n in args.num_outputs[i] if n is not None])\n",
    "         \n",
    "\n",
    "        if args.RNN[i] == \"LSTM\":\n",
    "            \n",
    "            self.rnn = nn.LSTMCell(args.RNN_emb_dim[i] * args.num_steps[i], args.RNN_dim[i])\n",
    "       \n",
    "        elif args.RNN == \"Customized\":\n",
    "            \n",
    "            self.rnn = nn.LSTMCell(args.RNN_emb_dim[i] * args.num_steps[i], args.RNN_dim[i])\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            raise NotImplementedError\n",
    "            \n",
    "        if not args.RNN_dropout[i] is None:\n",
    "            \n",
    "            self.drop = nn.Dropout(args.RNN_dropout[i])\n",
    "        \n",
    "        self.nets = nn.ModuleList()\n",
    "        \n",
    "        self.dist_types = []\n",
    "        \n",
    "        for o in args.num_outputs[i]:\n",
    "            \n",
    "            if not o is None:\n",
    "            \n",
    "                if o == 2:\n",
    "                    \n",
    "                    model = [\n",
    "                    nn.Linear(args.RNN_dim[i], args.RNN_dim[i]//4),\n",
    "                    nn.LayerNorm(args.RNN_dim[i]//4),\n",
    "                    nn.LeakyReLU(0.2, inplace = True),\n",
    "                    nn.Linear(args.RNN_dim[i]//4 , 1),\n",
    "                    ]\n",
    "                    self.dist_types.append(Bernoulli)\n",
    "\n",
    "                    model.append(nn.Sigmoid())\n",
    "\n",
    "                elif o > 2:\n",
    "                    \n",
    "                    model = [\n",
    "                    nn.Linear(args.RNN_dim[i], args.RNN_dim[i]//4),\n",
    "                    nn.LayerNorm(args.RNN_dim[i]//4),\n",
    "                    nn.LeakyReLU(0.2, inplace = True),\n",
    "                    nn.Linear(args.RNN_dim[i]//4 , o),\n",
    "                    ]\n",
    "                    \n",
    "                    self.dist_types.append(Categorical)\n",
    "\n",
    "                    if args.log_softmax[i]:\n",
    "\n",
    "                        model.append(nn.LogSoftmax(-1))\n",
    "                    else:\n",
    "                        model.append(nn.Softmax(-1))\n",
    "                else:\n",
    "                    \n",
    "                    raise NotImplementedError\n",
    "                \n",
    "                self.nets.append(nn.Sequential(*model))\n",
    "                \n",
    "        # Critic should be extra \n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(args.RNN_dim[i], args.RNN_dim[i]//4),\n",
    "                nn.LayerNorm(args.RNN_dim[i]//4),\n",
    "                nn.LeakyReLU(0.2, inplace = True),\n",
    "                nn.Linear(args.RNN_dim[i]//4 , 1),\n",
    "                )\n",
    "        \n",
    "    def forward(self, x, hidden = None):\n",
    "        \n",
    "        '''\n",
    "        # (Retrieve, opt(0~args.num_weight), BN(0~args.num_weight), Act_fn(0~args.num_act_fns), \n",
    "        Attn(0~args.num_Attn, Return, Put(0~args.num_putways-1))\n",
    "        \n",
    "        # Question : should I do the dropout in the controller.\n",
    "        \n",
    "        in short: [retr, opt, bn, act, attn, retur, put, output_signal]\n",
    "        \n",
    "        should I create 4 different forward function? \n",
    "        '''\n",
    "        \n",
    "        emb_out = []\n",
    "        \n",
    "        emb_out = [self.embs[j](x[:,j]) for j in range(x.size(1))]\n",
    "\n",
    "        rnn_input = torch.cat(emb_out,1)\n",
    "        \n",
    "        h, c = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        if not args.RNN_dropout[self.i] is None:\n",
    "\n",
    "            h = self.drop(h)\n",
    "        \n",
    "        dists = [self.dist_types[j](net(h)) for j, net in enumerate(self.nets)]\n",
    "\n",
    "        value = self.critic(h)\n",
    "        \n",
    "        return dists , (h,c), value\n",
    "    \n",
    "    \n",
    "    def sample(self, x, hidden = None, with_detials = False):\n",
    "        \n",
    "        dists, hidden, value = self.forward(x, hidden)\n",
    "        \n",
    "        actions = []\n",
    "        \n",
    "        for dist in dists:\n",
    "            \n",
    "            if type(dist) is Bernoulli:\n",
    "                actions.append(dist.sample()[:,0].long())\n",
    "            elif type(dist) is Categorical:\n",
    "                actions.append(dist.sample())\n",
    "            else: \n",
    "                raise NotImplementedError\n",
    "\n",
    "        samples = torch.stack(actions, 1)\n",
    "        \n",
    "        \n",
    "        if with_detials:\n",
    "            \n",
    "            entropies = []\n",
    "            log_probs = []\n",
    "            \n",
    "            for j, dist in enumerate(dists):\n",
    "                \n",
    "                if type(dist) is Bernoulli:\n",
    "                    entropies.append(dist.entropy()[:,0])\n",
    "                    log_probs.append(dist.log_prob(actions[j].float())[:,0])\n",
    "                elif type(dist) is Categorical:\n",
    "                    entropies.append(dist.entropy())\n",
    "                    log_probs.append(dist.log_prob(actions[j]))\n",
    "                else: \n",
    "                    raise NotImplementedError\n",
    "                    \n",
    "            args.e = entropies\n",
    "            args.l = log_probs\n",
    "            entropies = torch.stack(entropies, -1)\n",
    "            log_probs = torch.stack(log_probs, 1)\n",
    "\n",
    "            return samples, hidden, entropies, log_probs, value\n",
    "        \n",
    "        return samples, hidden\n",
    "    \n",
    "    def generate_dags(self, samples):\n",
    "        \n",
    "        # [retr, opt, bn, act, attn, retur, put]\n",
    "        \n",
    "        # SE&TE -> [Re, Re1, Re2, Re3, Op, Op1, Op2, Op3, Attn, Norm, Act_fn, Put, Return, Output] # no Out_opt \n",
    "        # ThinkingCell -> -> [Re1, Re2, Re3, Op, Op1, Op2, Op3, Attn, Norm, Act_fn, Put, Return, Output] # no Re, and Out_opt\n",
    "        # Decoder -> -> [Re, Re1, Re2, Re3, Op, Op1, Op2, Op3, Attn, Norm, Act_fn, Put, Return, Output, Out_opt]\n",
    "        \n",
    "        dags = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            \n",
    "            dag_dict = {}\n",
    "            \n",
    "            for j, name in enumerate(args.step_names[self.i]):\n",
    "                    \n",
    "                \n",
    "                if all( k == -1 for k in [name.find(n) for n in args.none_opt_names]) or (self.i == 2 and name == 'Attn'): # if it doesn't have None operation\n",
    "                    \n",
    "                    dag_dict[name] = sample[j].item()\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    if sample[j] == 0:\n",
    "                        \n",
    "                        dag_dict[name] = None\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        dag_dict[name] = sample[j].item() -1\n",
    "        \n",
    "            dags.append(dag_dict)\n",
    "            \n",
    "        return dags\n",
    "    \n",
    "    def straight_dags(self, x = None, hidden = None, batch_size = 1, with_details = False):\n",
    "\n",
    "        if x is None:\n",
    "            x = torch.ones(batch_size,args.num_steps[self.i]).to(device).long() # test the performance for ones and zeros\n",
    "        \n",
    "        sample_list = []\n",
    "        entropies = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        \n",
    "        for _ in range(args.num_steps[self.i]): \n",
    "            \n",
    "            if with_details:\n",
    "                \n",
    "                x, hidden, entropy, log_prob, value = self.sample(x, hidden, with_detials = True)\n",
    "                entropies.append(entropy)\n",
    "                log_probs.append(log_prob.mean())\n",
    "                values.append(value)\n",
    "                \n",
    "            else:\n",
    "                x, hidden = self.sample(x, hidden)  \n",
    "                    \n",
    "            sample_list.append(x)\n",
    "            \n",
    "            if x[0,args.step_names[self.i].index('Output')] == 1 and args.early_output[self.i]: # means the NN want to output this\n",
    "                break\n",
    "                \n",
    "        samples = torch.stack(sample_list,1)\n",
    "        \n",
    "        dags_dict_list = []\n",
    "        \n",
    "        for dag in samples:\n",
    "            dags_dict_list.append(self.generate_dags(dag))\n",
    "            \n",
    "        if with_details: \n",
    "            \n",
    "            args.en = entropies\n",
    "            \n",
    "            args.lo = log_probs\n",
    "            \n",
    "            args.va = values\n",
    "            return dags_dict_list, hidden, (torch.stack(entropies,1).sum(-1).squeeze().view(-1,1),\n",
    "                                            torch.stack(log_probs,0).view(-1,1),\n",
    "                                            torch.stack(values,1).squeeze().view(-1,1))\n",
    "\n",
    "        return dags_dict_list, hidden, None \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        \n",
    "        '''\n",
    "        This three controllers can be combined to generate 4 different dags.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.SE_controller = Controller(0)\n",
    "        self.TE_controller = Controller(1)\n",
    "        self.Th_controller = Controller(2)\n",
    "        self.De_controller = Controller(3)\n",
    "\n",
    "        self.h_net = nn.Linear(args.RNN_dim[0] + args.RNN_dim[1], args.RNN_dim[2])\n",
    "        self.c_net = nn.Linear(args.RNN_dim[0] + args.RNN_dim[1], args.RNN_dim[2])\n",
    "        \n",
    "        self.h_net_2 = nn.Linear(args.RNN_dim[2], args.RNN_dim[3])\n",
    "        self.c_net_2 = nn.Linear(args.RNN_dim[2], args.RNN_dim[3])\n",
    "        \n",
    "    def forward(self, with_details = False):\n",
    "        \n",
    "        d_ = []\n",
    "        \n",
    "        SE_dags, SE_hidden, details = self.SE_controller.straight_dags(with_details = with_details)\n",
    "        args.da = details\n",
    "        \n",
    "        d_.append(details)\n",
    "        \n",
    "        TE_dags, TE_hidden, details  = self.TE_controller.straight_dags(with_details = with_details)\n",
    "        args.db = details\n",
    "        \n",
    "        d_.append(details)\n",
    "        \n",
    "        if type(SE_hidden) is tuple:\n",
    "        \n",
    "            hidden = [ torch.cat([S,T],-1) for S, T in zip(SE_hidden,TE_hidden )]\n",
    "            h_ = self.h_net(hidden[0])\n",
    "            c_ = self.c_net(hidden[1])\n",
    "            Th_input_hidden = (h_, c_)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            hidden = torch.cat([SE_hidden, TE_hidden], -1)\n",
    "            Th_input_hidden = self.h_net(hidden[0])\n",
    "\n",
    "        Th_dags, Th_hidden, details = self.Th_controller.straight_dags(x = None, hidden = Th_input_hidden, with_details = with_details)\n",
    "        \n",
    "        args.dc = details\n",
    "        \n",
    "        d_.append(details)\n",
    "\n",
    "        if type(Th_hidden) is tuple:\n",
    "            De_input_hidden = (self.h_net_2(Th_hidden[0]), self.c_net_2(Th_hidden[1]))\n",
    "        else:\n",
    "            De_input_hidden = self.h_net_2(Th_hidden)\n",
    "        \n",
    "        De_dags, De_hidden, details = self.De_controller.straight_dags(x = None, hidden = De_input_hidden, with_details = with_details)\n",
    "        \n",
    "        args.dd = details\n",
    "        \n",
    "        d_.append(details)\n",
    "        \n",
    "        args.d_ = d_\n",
    "        \n",
    "        return (SE_dags[0], TE_dags[0], Th_dags[0], De_dags[0]), details\n",
    "                \n",
    "        \n",
    "        # they will have the same batch_szie, \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThinkingNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.SE = Encoder(0)\n",
    "        self.TE = Encoder(1)\n",
    "        self.Th = ThinkingCell(2)\n",
    "        self.De = Decoder(3)\n",
    "        \n",
    "        self.models = [self.SE, self.TE, self.Th, self.De]\n",
    "        \n",
    "        \n",
    "        self.SE_emb = nn.Embedding(args.S_vocab_size, args.emb_dim, padding_idx=Constants.PAD)\n",
    "        self.TE_emb = nn.Embedding(args.T_vocab_size, args.emb_dim, padding_idx=Constants.PAD)\n",
    "        \n",
    "    def forward(self, src_seq, trg_seq, dags):\n",
    "        \n",
    "        trg_seq = trg_seq[:, :-1]\n",
    "        \n",
    "        (SE_dags, TE_dags, Th_dags, De_dags) = dags\n",
    "        \n",
    "        \n",
    "        src_seq = self.SE_emb(src_seq)\n",
    "        trg_seq = self.TE_emb(trg_seq)\n",
    "        \n",
    "        SE_out = self.SE(src_seq, dags = SE_dags)\n",
    "        TE_out = self.TE(trg_seq, dags = TE_dags)\n",
    "        Th_out = self.Th(SE_out, TE_out, Th_dags)\n",
    "        De_out = self.De(TE_out, De_dags)\n",
    "        \n",
    "        \n",
    "        return De_out.view(-1, De_out.size(2))\n",
    "    \n",
    "    \n",
    "    def get_loss(self, loader, dags):\n",
    "        \n",
    "        batch = next(iter(loader))\n",
    "        \n",
    "        src_seq = batch.src[0].t().to(device)\n",
    "        trg_seq = batch.trg[0].t().to(device)\n",
    "        \n",
    "        gold = trg_seq[:,1:]\n",
    "        \n",
    "        pred = self.forward(src_seq, trg_seq, dags)\n",
    "        \n",
    "        non_pad_mask = gold.contiguous().view(-1).ne(Constants.PAD)\n",
    "        \n",
    "        n_word_total = non_pad_mask.sum().item()\n",
    "        \n",
    "        # the loss without smoothing\n",
    "        \n",
    "        loss = cal_loss(pred, gold.contiguous().view(-1), args.label_smooth)/n_word_total # what if the len of the pred and the trg_seq are not the same? \n",
    "        \n",
    "        pred = pred.max(1)[1]\n",
    "        n_correct = pred.eq(gold.contiguous().view(-1))\n",
    "        n_correct = n_correct.masked_select(non_pad_mask).sum()\n",
    "        \n",
    "        return loss, n_correct / n_word_total\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetchingNAS(nn.Module):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.controller = Agent()\n",
    "        self.shared = ThinkingNet()\n",
    "\n",
    "        self.training_loader = args.train_iter\n",
    "        self.valid_loader = args.val_iter\n",
    "        \n",
    "        self.shared_optim = optim.Adam(self.shared.parameters(), lr = args.shared_lr)\n",
    "        self.controller_optim = optim.Adam(self.controller.parameters(), lr = args.controller_lr)\n",
    "        \n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        self.max_R = None\n",
    "        self.best_dag = None\n",
    "        self.controller_hist = defaultdict(list)\n",
    "        \n",
    "        self.apply(self.weight_init)\n",
    "        \n",
    "    def train_shared(self,):\n",
    "        \n",
    "        self.shared.train()\n",
    "        self.controller.eval()\n",
    "        \n",
    "        step = 0\n",
    "        \n",
    "        while step < args.shared_maxsteps:\n",
    "            \n",
    "            self.shared_optim.zero_grad()\n",
    "            \n",
    "            dags, details = self.controller()\n",
    "            \n",
    "            loss, acc = self.shared.get_loss(self.training_loader, dags)\n",
    "            \n",
    "            loss.backward()\n",
    "            if not args.shared_grad_norm is None:\n",
    "                \n",
    "                nn.utils.clip_grad_norm_(self.shared.parameters(), args.shared_grad_norm)\n",
    "            \n",
    "            self.shared_optim.step()\n",
    "            \n",
    "            step += 1 \n",
    "    \n",
    "    def get_reward(self, dags, entropies,):\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(args.val_test_steps):\n",
    "            \n",
    "            l_, acc = self.shared.get_loss(self.valid_loader, dags)\n",
    "            \n",
    "            loss += l_\n",
    "            \n",
    "        loss /= args.val_test_steps\n",
    "        \n",
    "        loss_ppl = loss.exp()\n",
    "        \n",
    "        R = (args.reward_c / ((loss_ppl ** 2) + args.eps)) # * torch.ones_like(entropies)\n",
    "        \n",
    "        return R\n",
    "        \n",
    "        # If we sample a dag a time, then change the structure of sampling dag    \n",
    "            \n",
    "    def train_controller(self,):\n",
    "        \n",
    "        self.controller.train()\n",
    "        \n",
    "        step = 0\n",
    "        \n",
    "#         baseline = None\n",
    "\n",
    "        self.en_losses = []\n",
    "    \n",
    "        self.controller_losses = []\n",
    "        \n",
    "        self.actor_losses = []\n",
    "        \n",
    "        self.critic_losses = []\n",
    "        \n",
    "        while step < args.controller_maxsteps:\n",
    "            \n",
    "            self.controller_optim.zero_grad()\n",
    "            \n",
    "            dags, (entropy, log_prob, value) = self.controller(with_details = True)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                R = self.get_reward(dags, entropy)\n",
    "                \n",
    "            if self.max_R is None:\n",
    "                self.max_R = R.max()\n",
    "                self.best_dag = dags\n",
    "            else:\n",
    "                if R.max() > self.max_R:\n",
    "                    self.max_R = R.max()\n",
    "                    self.best_dag = dags\n",
    "             \n",
    "            self.controller_hist['Rewards'].append(R.item())\n",
    "            \n",
    "            returns = compute_gae(R, value)\n",
    "            \n",
    "            adv = returns.detach() - value \n",
    "            \n",
    "            actor_loss = (- log_prob * adv).mean()\n",
    "            \n",
    "            critic_loss = args.critic_coef * adv.pow(2).mean() \n",
    "            \n",
    "            en_loss = args.entropy_coef * entropy.mean()\n",
    "            \n",
    "            loss = actor_loss + critic_loss - en_loss\n",
    "            \n",
    "            self.actor_losses.append(actor_loss.item())\n",
    "            self.critic_losses.append(critic_loss.item())\n",
    "            self.en_losses.append(-en_loss.item())\n",
    "            self.controller_losses.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if not args.controller_grad_norm is None:\n",
    "                nn.utils.clip_grad_norm_(self.controller.parameters(), args.controller_grad_norm)\n",
    "            \n",
    "            self.controller_optim.step()\n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "    \n",
    "    def train(self, n_epoch, save_freq = 30):\n",
    "        \n",
    "        start_t = time.time()\n",
    "\n",
    "        for self.epoch in range(n_epoch):\n",
    "            \n",
    "            self.train_shared()\n",
    "            \n",
    "            # Trian the shared model's parameter for shared_maxsteps\n",
    "\n",
    "            self.train_controller()\n",
    "            \n",
    "            self.plot_controller_hist('_Rewards')\n",
    "            \n",
    "            end_t = time.time()\n",
    "\n",
    "            self.dags_df(dags_translate(self.best_dag))\n",
    "            \n",
    "            print('Epoch : [%d] Lasting Time: [%.4f]'% (self.epoch, end_t - start_t))\n",
    "            print('Loss | Controller [%.4f] | Actor [%.4f] | Critic [%.4f] | Entropy [%.4f]' %\\\n",
    "                   (sum(self.controller_losses)/ len(self.controller_losses),\n",
    "                   (sum(self.actor_losses)/len(self.actor_losses)),\n",
    "                   (sum(self.critic_losses)/len(self.critic_losses)),\n",
    "                   (sum(self.en_losses)/len(self.en_losses))\n",
    "                  ))\n",
    "            \n",
    "            if self.epoch % save_freq == 0 and self.epoch != 0:\n",
    "                self.model_save('Training')\n",
    "            # Train controller for controler_maxsteps\n",
    "            \n",
    "    def plot_controller_hist(self, step):\n",
    "        \n",
    "        clear_output(True)\n",
    "        fig, ax = plt.subplots(figsize= (20,8))\n",
    "        for name in self.controller_hist.keys():\n",
    "            plt.plot(self.controller_hist[name], label = name)\n",
    "            plt.xlabel('Number of Steps',fontsize=15)\n",
    "            plt.ylabel( name, fontsize=15)\n",
    "            plt.title(name, fontsize=30, fontweight =\"bold\")\n",
    "            plt.legend(loc = 'upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        fig.savefig(\"Train_Hist\"+str(step)+\".png\")\n",
    "    \n",
    "    def num_all_params(self,):\n",
    "        return sum([param.nelement() for param in self.parameters()])\n",
    "    \n",
    "    def model_save(self, step):\n",
    "        \n",
    "        path = args.model_path + args.model_name+'_Step_' + str(step) + '.pth'\n",
    "        torch.save({args.model_name: self.state_dict()}, path)\n",
    "        print('Model Saved')\n",
    "        \n",
    "    def load_step_dict(self, step):\n",
    "        \n",
    "        path = args.model_path + args.model_name +'_Step_' + str(step) + '.pth'\n",
    "        self.load_state_dict(torch.load(path, map_location = lambda storage, loc: storage)[args.model_name])\n",
    "        print('Model Loaded')\n",
    "        \n",
    "    def dags_df(self, dags):\n",
    "        \n",
    "        args.translated_dag = dags\n",
    "         \n",
    "        for dag, name in zip(dags, args.names):\n",
    "            \n",
    "            df = pd.DataFrame(dag)\n",
    "            \n",
    "            print('\\n\\n'+name)\n",
    "            \n",
    "            display(df)\n",
    "    \n",
    "    def weight_init(self,m):\n",
    "        \n",
    "        if type(m) in [nn.Conv2d, nn.ConvTranspose2d, nn.Linear]:\n",
    "            nn.init.kaiming_normal_(m.weight,0.2,nonlinearity='leaky_relu')\n",
    "        elif type(m) in [nn.LSTM]:\n",
    "            for name, value in m.named_parameters():\n",
    "                if 'weight' in name :\n",
    "                    nn.init.xavier_normal_(value.data)\n",
    "                if 'bias'in name:\n",
    "                    value.data.normal_()\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fet = FetchingNAS().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SourceEncoder\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Act_fn</th>\n",
       "      <th>Attn</th>\n",
       "      <th>Norm</th>\n",
       "      <th>Operation</th>\n",
       "      <th>Operation_1</th>\n",
       "      <th>Operation_2</th>\n",
       "      <th>Operation_3</th>\n",
       "      <th>Output</th>\n",
       "      <th>Put</th>\n",
       "      <th>Retrieve_cell</th>\n",
       "      <th>Retrieve_cell_1</th>\n",
       "      <th>Retrieve_cell_2</th>\n",
       "      <th>Retrieve_cell_3</th>\n",
       "      <th>Return_cell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Emlement_Wise*</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Softmax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>Inplace</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tanh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>Emlement_Wise*</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Softmax</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>Inplace</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>Emlement_Wise*</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Softplus</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Inplace</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>Emlement_Wise*</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Softmax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Inplace</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Act_fn  Attn  Norm  Operation  Operation_1  Operation_2  Operation_3  \\\n",
       "0  LeakyReLU   4.0     7        0.0            3            1            0   \n",
       "1    Softmax   NaN     4        6.0            2            8            6   \n",
       "2       Tanh   0.0     5        7.0            7            0            8   \n",
       "3    Softmax   4.0     3        NaN            5            5            7   \n",
       "4  LeakyReLU   2.0     9        9.0            6            3            6   \n",
       "5   Softplus   1.0     9        0.0            5            3            0   \n",
       "6       None   1.0     4        5.0            7            5            7   \n",
       "7    Softmax   0.0     4        NaN            8            1            3   \n",
       "\n",
       "   Output             Put  Retrieve_cell  Retrieve_cell_1  Retrieve_cell_2  \\\n",
       "0   False  Emlement_Wise*              7                4                3   \n",
       "1   False         Inplace              0                9                1   \n",
       "2   False  Emlement_Wise*              7                4                5   \n",
       "3   False         Inplace              0                2                4   \n",
       "4   False  Emlement_Wise*              8                4                1   \n",
       "5   False         Inplace              8                3                3   \n",
       "6   False  Emlement_Wise*              6                4                5   \n",
       "7    True         Inplace              6                6                3   \n",
       "\n",
       "   Retrieve_cell_3  Return_cell  \n",
       "0                4            7  \n",
       "1                2            7  \n",
       "2                7            8  \n",
       "3                2            7  \n",
       "4                5            6  \n",
       "5                0            3  \n",
       "6                1            7  \n",
       "7                4            8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TargetEncoder\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Act_fn</th>\n",
       "      <th>Attn</th>\n",
       "      <th>Norm</th>\n",
       "      <th>Operation</th>\n",
       "      <th>Operation_1</th>\n",
       "      <th>Operation_2</th>\n",
       "      <th>Operation_3</th>\n",
       "      <th>Output</th>\n",
       "      <th>Put</th>\n",
       "      <th>Retrieve_cell</th>\n",
       "      <th>Retrieve_cell_1</th>\n",
       "      <th>Retrieve_cell_2</th>\n",
       "      <th>Retrieve_cell_3</th>\n",
       "      <th>Return_cell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>Plus</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Act_fn  Attn  Norm  Operation Operation_1 Operation_2  Operation_3  \\\n",
       "0  Sigmoid     1     8          2        None        None            9   \n",
       "\n",
       "   Output   Put  Retrieve_cell  Retrieve_cell_1  Retrieve_cell_2  \\\n",
       "0    True  Plus              2                6                5   \n",
       "\n",
       "   Retrieve_cell_3  Return_cell  \n",
       "0                7            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ThinkingCell\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Act_fn</th>\n",
       "      <th>Attn</th>\n",
       "      <th>Norm</th>\n",
       "      <th>Operation</th>\n",
       "      <th>Operation_1</th>\n",
       "      <th>Operation_2</th>\n",
       "      <th>Operation_3</th>\n",
       "      <th>Output</th>\n",
       "      <th>Put</th>\n",
       "      <th>Retrieve_cell_1</th>\n",
       "      <th>Retrieve_cell_2</th>\n",
       "      <th>Retrieve_cell_3</th>\n",
       "      <th>Return_cell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>Plus</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Act_fn  Attn  Norm  Operation  Operation_1  Operation_2  Operation_3  \\\n",
       "0  Sigmoid     4     4          7            7            3            9   \n",
       "\n",
       "   Output   Put  Retrieve_cell_1  Retrieve_cell_2  Retrieve_cell_3  \\\n",
       "0    True  Plus                1                3                2   \n",
       "\n",
       "   Return_cell  \n",
       "0            1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Decoder\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Act_fn</th>\n",
       "      <th>Attn</th>\n",
       "      <th>Norm</th>\n",
       "      <th>Operation</th>\n",
       "      <th>Operation_1</th>\n",
       "      <th>Operation_2</th>\n",
       "      <th>Operation_3</th>\n",
       "      <th>Out_opt</th>\n",
       "      <th>Output</th>\n",
       "      <th>Put</th>\n",
       "      <th>Retrieve_cell</th>\n",
       "      <th>Retrieve_cell_1</th>\n",
       "      <th>Retrieve_cell_2</th>\n",
       "      <th>Retrieve_cell_3</th>\n",
       "      <th>Return_cell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ReLU</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>Plus</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Act_fn  Attn  Norm  Operation  Operation_1  Operation_2  Operation_3  \\\n",
       "0   ReLU     3     9         13           13            8            4   \n",
       "\n",
       "   Out_opt  Output   Put  Retrieve_cell  Retrieve_cell_1  Retrieve_cell_2  \\\n",
       "0        2    True  Plus              4                3                2   \n",
       "\n",
       "   Retrieve_cell_3  Return_cell  \n",
       "0                2            1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Lasting Time: [28.4078]\n",
      "Loss | Controller [-1.0749] | Actor [-1.9698] | Critic [0.8950] | Entropy [0.0000]\n"
     ]
    }
   ],
   "source": [
    "Fet.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
